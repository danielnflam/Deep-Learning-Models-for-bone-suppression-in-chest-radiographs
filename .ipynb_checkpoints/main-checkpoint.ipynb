{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c6f3875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os, sys, datetime, time, random, fnmatch, math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import skimage.metrics\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms as tvtransforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, ConcatDataset\n",
    "import torchvision.utils as vutils\n",
    "import torch.utils.tensorboard as tensorboard\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datasets, custom_transforms, GusarevModel, pytorch_msssim\n",
    "\n",
    "flag_debug = False\n",
    "flag_load_previous_save = False\n",
    "\n",
    "# Input Directories\n",
    "#data_BSE = \"D:/data/JSRT/BSE_JSRT\"\n",
    "#data_normal = \"D:/data/JSRT/JSRT\"\n",
    "data_BSE = \"G:/DanielLam/JSRT/HQ_JSRT_and_BSE-JSRT/177-20-20 split/train_augmented_noEqualise/suppressed/\"\n",
    "data_normal = \"G:/DanielLam/JSRT/HQ_JSRT_and_BSE-JSRT/177-20-20 split/train_augmented_noEqualise/normal/\"\n",
    "\n",
    "data_val_normal = 'G:/DanielLam/JSRT/HQ_JSRT_and_BSE-JSRT/177-20-20 split/validate/normal'\n",
    "data_val_BSE = 'G:/DanielLam/JSRT/HQ_JSRT_and_BSE-JSRT/177-20-20 split/validate/suppressed/'\n",
    "\n",
    "# Save directories:\n",
    "output_save_directory = Path(\"./runs/6LayerCNN/177-20-20/10KFold\")\n",
    "output_save_directory.mkdir(parents=True, exist_ok=True)\n",
    "PATH_SAVE_NETWORK_INTERMEDIATE = os.path.join(output_save_directory, 'network_intermediate_{}.tar' )\n",
    "PATH_SAVE_NETWORK_FINAL = os.path.join(output_save_directory, 'network_final_{}.pt')\n",
    "\n",
    "# Image Size:\n",
    "image_spatial_size = (256 , 256)\n",
    "_batch_size = 5\n",
    "split_k_folds=10\n",
    "\n",
    "\n",
    "# Optimisation\n",
    "lr_ini = 0.001 \n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "\n",
    "# Training\n",
    "num_reals_per_epoch_paper = 4000 # in Gusarev et al. 2017\n",
    "total_num_epochs_paper = 150\n",
    "num_epochs_decay_lr_paper = 100\n",
    "lr_decay_ratio = 0.25\n",
    "\n",
    "# Weight Initialisation\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.normal_(m.weight.data, 0., 0.02)\n",
    "        #nn.init.kaiming_normal_(m.weight.data,0)\n",
    "        try:\n",
    "            nn.init.constant_(m.bias.data, 0.)\n",
    "        except:\n",
    "            pass\n",
    "    if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.InstanceNorm2d):\n",
    "        if m.affine:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0.)\n",
    "\n",
    "## Code for putting things on the GPU\n",
    "ngpu = 1 #torch.cuda.device_count()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(device)\n",
    "if (torch.cuda.is_available()):\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    if ngpu ==1:\n",
    "        device=torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "807bc8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current date:\n",
    "current_date=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Data Loader\n",
    "original_key = \"source\"\n",
    "target_key = \"boneless\"\n",
    "discriminator_keys_images = [original_key, target_key]\n",
    "ds_training = datasets.JSRT_CXR(data_normal, data_BSE,\n",
    "                         transform=tvtransforms.Compose([\n",
    "                             # custom_transforms.HistogramEqualisation(discriminator_keys_images),-- check if training data is already equalised\n",
    "                             custom_transforms.Resize(discriminator_keys_images, image_spatial_size),\n",
    "                             custom_transforms.ToTensor(discriminator_keys_images),\n",
    "                             ])\n",
    "                      )\n",
    "ds_val = datasets.JSRT_CXR(data_val_normal, data_val_BSE,\n",
    "                         transform=tvtransforms.Compose([\n",
    "                             #custom_transforms.HistogramEqualisation(discriminator_keys_images),\n",
    "                             custom_transforms.Resize(discriminator_keys_images, image_spatial_size),\n",
    "                             custom_transforms.ToTensor(discriminator_keys_images),\n",
    "                             ])\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "650c4d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss definitions\n",
    "\n",
    "# Gusarev Loss\n",
    "def criterion_MSELoss(testImage, referenceImage):\n",
    "    mse = nn.MSELoss()\n",
    "    mse_loss = mse(testImage, referenceImage)\n",
    "    return mse_loss, mse_loss, torch.zeros(1)\n",
    "def criterion_Gusarev(testImage, referenceImage, alpha=0.84):\n",
    "    \"\"\"\n",
    "    Gusarev et al. 2017. Deep learning models for bone suppression in chest radiographs.  IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology.\n",
    "    \"\"\"\n",
    "    mse = nn.MSELoss() # L2 used for easier optimisation c.f. L1\n",
    "    mse_loss = mse(testImage, referenceImage)\n",
    "    msssim = pytorch_msssim.MSSSIM(window_size=11, size_average=True, channel=1, normalize='relu')\n",
    "    msssim_loss = 1 - msssim(testImage, referenceImage)\n",
    "    total_loss = (1-alpha)*mse_loss + alpha*msssim_loss\n",
    "    return total_loss, mse_loss, msssim_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09474939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boneless\n",
      "Fold 1\n",
      "Using 6-Layer MultiCNN Model.\n",
      "Epoch Factor: 1\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "FLAG: NO CHECKPOINT LOADED.\n",
      "0.001\n",
      "[0/150][0/737]\tTotal Loss: 0.8641\tMSELoss: 0.1508\tMSSSIM Loss: 1.0000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fixed_val_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-955c995a8e31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreals_shown_now\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                     \u001b[0mval_cleaned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfixed_val_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"source\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Printing to img_list\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                 \u001b[0mimg_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_cleaned\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fixed_val_sample' is not defined"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# K-FOLD VALIDATION\n",
    "dataset = ConcatDataset([ds_training, ds_val])\n",
    "splits=KFold(n_splits=split_k_folds,shuffle=True,random_state=42)\n",
    "foldperf={}\n",
    "\n",
    "print(target_key)\n",
    "\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    history = {'loss':[], 'ssim_acc':[]}\n",
    "    \n",
    "    # Subset sample from dataset\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    dl_training = DataLoader(dataset, batch_size=_batch_size, sampler=train_sampler, num_workers=0)\n",
    "    dl_validation = DataLoader(dataset, batch_size=_batch_size, sampler=test_sampler, num_workers=0)\n",
    "    \n",
    "    # Network\n",
    "    input_array_size = (_batch_size, 1, image_spatial_size[0], image_spatial_size[1])\n",
    "    net = GusarevModel.MultilayerCNN(input_array_size)\n",
    "    # Initialise weights\n",
    "    net.apply(weights_init)\n",
    "\n",
    "    # Multi-GPU\n",
    "    if (device.type == 'cuda') and (ngpu > 1):\n",
    "        print(\"Neural Net on GPU\")\n",
    "        net = nn.DataParallel(net, list(range(ngpu)))\n",
    "    net = net.to(device)\n",
    "\n",
    "    # Optimiser\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr_ini, betas=(beta1, beta2))\n",
    "\n",
    "    # Learning Rate Scheduler\n",
    "    epoch_factor = round(num_reals_per_epoch_paper/len(ds_training)) # need to have this factor as many epochs as that described in the paper\n",
    "    print(\"Epoch Factor: \"+str(epoch_factor))\n",
    "    total_num_epochs = int(total_num_epochs_paper*epoch_factor)\n",
    "    num_epochs_decay_lr = int(num_epochs_decay_lr_paper*epoch_factor)\n",
    "    lambda_rule = lambda epoch: 1*((1-lr_decay_ratio)**(epoch//num_epochs_decay_lr))\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule, verbose=True)\n",
    "    \n",
    "    # For each epoch\n",
    "    epochs_list = []\n",
    "    img_list = []\n",
    "    training_loss_list = []\n",
    "    reals_shown = []\n",
    "    #validation_loss_per_epoch_list = []\n",
    "    #training_loss_per_epoch_list = []\n",
    "    loss_per_epoch={\"training\":[], \"validation\":[]}\n",
    "    ssim_average={\"training\":[], \"validation\":[]}\n",
    "\n",
    "    \n",
    "    \n",
    "    #fixed_val_sample = next(iter(dl_validation))\n",
    "    #fig, ax = plt.subplots(1,2)\n",
    "    #ax[0].imshow(fixed_val_sample[original_key][0,0,:])\n",
    "    #ax[1].imshow(fixed_val_sample[target_key][0,0,:])\n",
    "    #plt.show()\n",
    "    \n",
    "    # optionally resume from a checkpoint\n",
    "    if flag_load_previous_save:\n",
    "        if os.path.isfile(PATH_SAVE_NETWORK_INTERMEDIATE):\n",
    "            print(\"=> loading checkpoint '{}'\".format(PATH_SAVE_NETWORK_INTERMEDIATE))\n",
    "            checkpoint = torch.load(PATH_SAVE_NETWORK_INTERMEDIATE)\n",
    "            start_epoch = checkpoint['epoch_next']\n",
    "            reals_shown_now = checkpoint['reals_shown']\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            net.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {}, reals shown {})\".format(PATH_SAVE_NETWORK_INTERMEDIATE, \n",
    "                                                                                start_epoch, reals_shown_now))\n",
    "            print(scheduler)\n",
    "        else:\n",
    "            print(\"=> NO CHECKPOINT FOUND AT '{}'\" .format(PATH_SAVE_NETWORK_INTERMEDIATE))\n",
    "            raise RuntimeError(\"No checkpoint found at specified path.\")\n",
    "    else:\n",
    "        print(\"FLAG: NO CHECKPOINT LOADED.\")\n",
    "        reals_shown_now = 0\n",
    "        start_epoch=0\n",
    "\n",
    "    # Loop variables\n",
    "    flag_break = False # when debugging, this will automatically go to True\n",
    "    iters = 0\n",
    "    net.train()\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = True\n",
    "    for epoch in range(start_epoch, total_num_epochs):\n",
    "        print(optimizer.param_groups[0]['lr'])\n",
    "        sum_loss_in_epoch = 0\n",
    "        for i, data in enumerate(dl_training):\n",
    "            # Training\n",
    "            net.zero_grad()\n",
    "            noisy_data = data[original_key].to(device)\n",
    "            cleaned_data = net(noisy_data)\n",
    "            loss, maeloss, msssim_loss = criterion_Gusarev(cleaned_data, data[target_key].to(device))\n",
    "            loss.backward() # calculate gradients\n",
    "            optimizer.step() # optimiser step along gradients\n",
    "\n",
    "            # Output training stats\n",
    "            if i % 200 == 0:\n",
    "                print('[%d/%d][%d/%d]\\tTotal Loss: %.4f\\tMSELoss: %.4f\\tMSSSIM Loss: %.4f'\n",
    "                      % (epoch, total_num_epochs, i, len(dl_training),\n",
    "                         loss.item(), maeloss.item(), msssim_loss.item()))\n",
    "            # Record generator output\n",
    "            #if reals_shown_now%(100*_batch_size)==0:\n",
    "            #    with torch.no_grad():\n",
    "            #        val_cleaned = net(fixed_val_sample[\"source\"].to(device)).detach().cpu()\n",
    "            #    print(\"Printing to img_list\")\n",
    "            #    img_list.append(vutils.make_grid(val_cleaned[0:1,0:1,:], padding=2, normalize=True))\n",
    "            #iters +=1\n",
    "            #if flag_debug and iters>=10:\n",
    "            #    flag_break = True\n",
    "            #    break\n",
    "\n",
    "            # Running counter of reals shown\n",
    "            reals_shown_now += _batch_size\n",
    "            reals_shown.append(reals_shown_now)\n",
    "\n",
    "            # Training loss list for loss per minibatch\n",
    "            training_loss_list.append(loss.item()) # training loss\n",
    "            sum_loss_in_epoch += loss.item()*len(cleaned_data)\n",
    "        # Turn the sum loss in epoch into a loss-per-epoch\n",
    "        loss_per_epoch[\"training\"].append(sum_loss_in_epoch/len(ds_training))\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Training Accuracy per EPOCH\n",
    "            ssim_training_list = []\n",
    "            for train_count, data in enumerate(dl_training):\n",
    "                noisy_training_data = data[original_key].to(device)\n",
    "                true_training_data = data[target_key]\n",
    "                cleaned_training_data = net(noisy_training_data)\n",
    "\n",
    "                for ii, image in enumerate(cleaned_training_data):\n",
    "                    clean_training_numpy = image.cpu().detach().numpy()\n",
    "                    true_training_numpy = true_training_data[ii].numpy()\n",
    "                    clean_training_numpy = np.moveaxis(clean_training_numpy, 0, -1)\n",
    "                    true_training_numpy = np.moveaxis(true_training_numpy, 0, -1)\n",
    "                    ssim_training = skimage.metrics.structural_similarity(clean_training_numpy, true_training_numpy, multichannel=True)\n",
    "                    ssim_training_list.append(ssim_training) # SSIM per image\n",
    "            ssim_average[\"training\"].append(np.mean(ssim_training_list))\n",
    "\n",
    "            # Validation Loss and Accuracy per EPOCH\n",
    "            sum_loss_in_epoch =0\n",
    "            ssim_val_list = []\n",
    "            for val_count, sample in enumerate(dl_validation):\n",
    "                noisy_val_data = sample[original_key].to(device)\n",
    "                cleaned_val_data = net(noisy_val_data)\n",
    "\n",
    "                # Loss\n",
    "                true_val_data = sample[target_key]\n",
    "                val_loss, maeloss, msssim_loss = criterion_Gusarev(cleaned_val_data, true_val_data.to(device))\n",
    "                sum_loss_in_epoch += val_loss.item()*len(cleaned_val_data)\n",
    "\n",
    "                # Accuracy\n",
    "                for ii, image in enumerate(cleaned_val_data):\n",
    "                    clean_val_numpy = image.cpu().detach().numpy()\n",
    "                    true_val_numpy = true_val_data[ii].numpy()\n",
    "                    clean_val_numpy = np.moveaxis(clean_val_numpy, 0, -1)\n",
    "                    true_val_numpy = np.moveaxis(true_val_numpy, 0, -1)\n",
    "                    ssim_val = skimage.metrics.structural_similarity(clean_val_numpy, true_val_numpy, multichannel=True)\n",
    "                    ssim_val_list.append(ssim_val) # SSIM per image\n",
    "            # After considering all validation images\n",
    "            loss_per_epoch[\"validation\"].append(sum_loss_in_epoch/len(ds_val))\n",
    "            ssim_average[\"validation\"].append(np.mean(ssim_val_list))\n",
    "        epochs_list.append(epoch)\n",
    "        # LR Scheduler after epoch\n",
    "        scheduler.step()\n",
    "        \n",
    "        #if epoch % 5 == 0:\n",
    "        #    if not flag_debug:\n",
    "        #        #torch.save(net.state_dict(), PATH_SAVE_NETWORK_INTERMEDIATE)\n",
    "        #        torch.save({\n",
    "        #        'epochs_completed': epoch+1,\n",
    "        #        'epoch_next': epoch+1,\n",
    "        #        'model_state_dict': net.state_dict(),\n",
    "        #        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #        'loss': loss,\n",
    "        #        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        #        'reals_shown': reals_shown_now\n",
    "        #        }, PATH_SAVE_NETWORK_INTERMEDIATE.format(fold))\n",
    "        #        print(\"Saved Intermediate: \"+ str(PATH_SAVE_NETWORK_INTERMEDIATE.format(fold))\n",
    "        #if flag_break:\n",
    "        #    break\n",
    "        \n",
    "    #After all epochs, save results:\n",
    "    history[\"loss\"] = loss_per_epoch\n",
    "    history[\"ssim_acc\"] = ssim_average\n",
    "    \n",
    "    foldperf['fold{}'.format(fold+1)] = history\n",
    "    \n",
    "    # Final Save for each fold\n",
    "    if not flag_debug:\n",
    "        torch.save(net.state_dict(), PATH_SAVE_NETWORK)\n",
    "        torch.save({\n",
    "                'epochs_completed': epoch+1,\n",
    "                'epoch_next': epoch+1,\n",
    "                'model_state_dict': net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'reals_shown': reals_shown_now\n",
    "                }, PATH_SAVE_NETWORK_INTERMEDIATE.format(fold))\n",
    "        print(\"Saved Intermediate: \"+ str(PATH_SAVE_NETWORK_INTERMEDIATE.format(fold)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c2d937-77e9-42af-9ca1-8691aa3f99a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2f546-5e04-4f24-9242-1517c3786836",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training complete\")\n",
    "\n",
    "testl_f,tl_f,testa_f,ta_f=[],[],[],[]\n",
    "k=10\n",
    "for f in range(1,k+1):\n",
    "    tl_f.append(np.mean(foldperf['fold{}'.format(f)]['loss']['training']))\n",
    "    testl_f.append(np.mean(foldperf['fold{}'.format(f)]['loss']['validation']))\n",
    "\n",
    "    ta_f.append(np.mean(foldperf['fold{}'.format(f)]['ssim_acc']['training']))\n",
    "    testa_f.append(np.mean(foldperf['fold{}'.format(f)]['ssim_acc']['validation']))\n",
    "\n",
    "print('Performance of {} fold cross validation'.format(k))\n",
    "print(\"Average Training Loss: {:.3f} \\t Average Test Loss: {:.3f} \\t Average Training Acc: {:.2f} \\t Average Test Acc: {:.2f}\"\n",
    "      .format(np.mean(tl_f),np.mean(testl_f),np.mean(ta_f),np.mean(testa_f)))\n",
    "\n",
    "# Averaging accuracy and loss\n",
    "diz_ep = {'train_loss_ep':[],'test_loss_ep':[],'train_acc_ep':[],'test_acc_ep':[]}\n",
    "for i in range(num_epochs):\n",
    "    diz_ep['train_loss_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['loss']['training'][i] for f in range(k)]))\n",
    "    diz_ep['test_loss_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['loss']['validation'][i] for f in range(k)]))\n",
    "    diz_ep['train_acc_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['ssim_acc']['training'][i] for f in range(k)]))\n",
    "    diz_ep['test_acc_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['ssim_acc']['validation'][i] for f in range(k)]))\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(diz_ep['train_loss_ep'], label='Train')\n",
    "plt.semilogy(diz_ep['test_loss_ep'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "if not flag_debug:\n",
    "    plt.savefig(os.path.join(output_save_directory, current_date + \"_loss\"+\".png\"))\n",
    "    \n",
    "# Plot accuracies\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(diz_ep['train_acc_ep'], label='Train')\n",
    "plt.semilogy(diz_ep['test_acc_ep'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('SSIM')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "if not flag_debug:\n",
    "    plt.savefig(os.path.join(output_save_directory, current_date + \"_accuracy\"+\".png\"))\n",
    "\n",
    "\n",
    "# CHECKPOINT SAVE\n",
    "#PATH_SAVE_NETWORK_FINAL = os.path.join(output_save_directory, 'network_final.tar' )\n",
    "#torch.save({\n",
    "#            'epochs_completed': epoch+1,\n",
    "#            'epoch_next': epoch+1,\n",
    "#            'model_state_dict': net.state_dict(),\n",
    "#            'optimizer_state_dict': optimizer.state_dict(),\n",
    "#            'scheduler_state_dict': scheduler.state_dict(),\n",
    "#            'reals_shown': reals_shown_now\n",
    "#            }, PATH_SAVE_NETWORK_FINAL)\n",
    "#print(\"Saved Checkpoint '{}' at epoch {}\".format(PATH_SAVE_NETWORK_FINAL, epoch+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67712b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "current_date=datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Accuracy\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"SSIM\")\n",
    "plt.plot(epochs_list, ssim_average[\"training\"], label='training')\n",
    "plt.plot(epochs_list, ssim_average[\"validation\"] , label='validation')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"SSIM\")\n",
    "plt.legend()\n",
    "#if not flag_debug:\n",
    "#    plt.savefig(os.path.join(output_save_directory, current_date + \"_accuracy\"+\".png\"))\n",
    "    \n",
    "# All losses\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Losses\")\n",
    "plt.plot(epochs_list, loss_per_epoch[\"training\"], label='training')\n",
    "plt.plot(epochs_list, loss_per_epoch[\"validation\"] , label='validation')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "#if not flag_debug:\n",
    "#    plt.savefig(os.path.join(output_save_directory, current_date + \"_loss\"+\".png\"))\n",
    "\n",
    "# Loss for training\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Loss during training\")\n",
    "plt.plot(reals_shown, [math.log10(y) for y in training_loss_list])\n",
    "plt.xlabel(\"reals_shown\")\n",
    "plt.ylabel(\"Training Log10(Loss)\")\n",
    "#if not flag_debug:\n",
    "#    plt.savefig(os.path.join(output_save_directory, current_date + \"_training_loss\"+\".png\"))\n",
    "\n",
    "\n",
    "    \n",
    "# Final Model:\n",
    "with torch.no_grad():\n",
    "    input_image = fixed_val_sample['source']\n",
    "    input_images = vutils.make_grid(input_image[0:1,:,:,:], padding=2, normalize=True)\n",
    "    target_images = vutils.make_grid(fixed_val_sample['boneless'][0:1,:,:,:], padding=2, normalize=True)\n",
    "    net = net.cpu()\n",
    "    output_image = net(input_image[0:1,:,:,:]).detach().cpu()\n",
    "    output_images = vutils.make_grid(output_image, padding=2, normalize=True)\n",
    "#print(str(torch.max(output_images)) + \",\" + str(torch.min(output_images)))\n",
    "plt.figure(1)\n",
    "fig, ax = plt.subplots(1,3, figsize=(15,15))\n",
    "ax[0].imshow(np.transpose(input_images, (1,2,0)), vmin=0, vmax=1)\n",
    "ax[0].set_title(\"Source\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(np.transpose(output_images, (1,2,0)), vmin=0, vmax=1)\n",
    "ax[1].set_title(\"Suppressed\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[2].imshow(np.transpose(target_images, (1,2,0)), vmin=0, vmax=1)\n",
    "ax[2].set_title(\"Ideal Bone-suppressed\")\n",
    "ax[2].axis(\"off\")\n",
    "plt.show\n",
    "#if not flag_debug:\n",
    "#    plt.savefig(os.path.join(output_save_directory, current_date + \"_validation_ComparisonImages\"+\".png\"))\n",
    "\n",
    "# ANIMATED VALIDATION IMAGE\n",
    "#fig = plt.figure(figsize=(8,8))\n",
    "#ax = fig.add_subplot(111)\n",
    "#plt.axis(\"off\")\n",
    "#ims = []\n",
    "#training_ims_shown = []\n",
    "#for i, im in enumerate(img_list):\n",
    "#    if i % 50 == 0:  # controls how many images are printed into the animation\n",
    "#        training_ims_shown = i*(100*_batch_size)\n",
    "#        frame = ax.imshow(np.transpose(im,(1,2,0)))\n",
    "#        t = ax.annotate(\"Reals shown: {}\".format(training_ims_shown), (0.5,1.02), xycoords=\"axes fraction\")\n",
    "#        ims.append([frame, t])\n",
    "#ani = animation.ArtistAnimation(fig, ims, interval=200, repeat_delay=1000, blit=True)\n",
    "#if not flag_debug:\n",
    "#    ani.save(os.path.join(output_save_directory, current_date+\"_animation.mp4\"), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7384b7a-734f-4cb8-8dd8-4fc75c599ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
